{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    " \n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The twitter_samples corpus contains 3 files. <br>\n",
    "\n",
    "1) negative_tweets.json: contains 5k negative tweets <br>\n",
    "2) positive_tweets.json: contains 5k positive tweets <br>\n",
    "3) tweets.20150430-223406.json: contains 20k positive and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "print (twitter_samples.fileids())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Positive Tweets = 5000\n",
      "Length of Negative Tweets = 5000\n",
      "Length of all Tweets sample = 20000\n"
     ]
    }
   ],
   "source": [
    " \n",
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "print (\"Length of Positive Tweets =\",len(pos_tweets)) # Output: 5000\n",
    " \n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "print (\"Length of Negative Tweets =\",len(neg_tweets)) # Output: 5000\n",
    " \n",
    "all_tweets = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "print (\"Length of all Tweets sample =\",len(all_tweets)) # Output: 20000\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's print some sample tweets to investigate the data\n",
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
      "@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
      "@97sides CONGRATS :)\n",
      "yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n"
     ]
    }
   ],
   "source": [
    "print(\"Let's print some sample tweets to investigate the data\")\n",
    "for tweet in pos_tweets[:5]:\n",
    "    print (tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Tweets\n",
    "NLTK has a TweetTokenizer module that does a good job in tokenizing (splitting text into a list of words) tweets.\n",
    "<br>\n",
    "Three different parameters can be passed while calling the TweetTokenizer class. They are:\n",
    "<br>\n",
    "``preserve_case: if False then it converts tweet to lowercase and vice-versa.\n",
    "strip_handles: if True then it removes twitter handles from the tweet and vice-versa.\n",
    "reduce_len: if True then it reduces the length of words in the tweet like hurrayyyy, yipppiieeee, etc. and vice-versa.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#followfriday', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n",
      "['hey', 'james', '!', 'how', 'odd', ':/', 'please', 'call', 'our', 'contact', 'centre', 'on', '02392441234', 'and', 'we', 'will', 'be', 'able', 'to', 'assist', 'you', ':)', 'many', 'thanks', '!']\n",
      "['we', 'had', 'a', 'listen', 'last', 'night', ':)', 'as', 'you', 'bleed', 'is', 'an', 'amazing', 'track', '.', 'when', 'are', 'you', 'in', 'scotland', '?', '!']\n",
      "['congrats', ':)']\n",
      "['yeaaah', 'yipppy', '!', '!', '!', 'my', 'accnt', 'verified', 'rqst', 'has', 'succeed', 'got', 'a', 'blue', 'tick', 'mark', 'on', 'my', 'fb', 'profile', ':)', 'in', '15', 'days']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    " \n",
    "for tweet in pos_tweets[:5]:\n",
    "    print (tweet_tokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Tweet\n",
    "In the tweet cleaning process, we will do the following:\n",
    "\n",
    "– Remove stock market tickers like \"$GE\"  <br>\n",
    "– Remove retweet text “RT” <br>\n",
    "– Remove hyperlinks <br>\n",
    "– Remove hashtags (only the hashtag # and not the word) <br>\n",
    "– Remove stop words like a, and, the, is, are, etc. <br>\n",
    "– Remove emoticons like :), :D, :(, :-), etc. <br> \n",
    "– Remove punctuation like full-stop, comma, exclamation sign, etc. <br> \n",
    "– Convert words to Stem/Base words using Porter Stemming Algorithm. E.g. words like ‘working’, ‘works’, and ‘worked’ will be converted to their base/stem word “work”. <br>\n",
    "\n",
    "We will define a function named clean_tweets which returns a list of cleaned (by removing the above-mentioned things) words for any given tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    " \n",
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english')\n",
    " \n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "from nltk.tokenize import TweetTokenizer\n",
    " \n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    " \n",
    "def clean_tweets(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    " \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "              word not in emoticons and # remove emoticons\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    " \n",
    "    return tweets_clean\n",
    " \n",
    "custom_tweet = \"RT @Twitter @finkd Hello There! Have a great day. :) #good #morning \"\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Tweets = ['hello', 'great', 'day', 'good', 'morn']\n",
      "Positive tweets =  @BhaktisBanter @PallaviRuhail This one is irresistible :)\n",
      "#FlipkartFashionFriday http://t.co/EbZ0L2VENM\n",
      "Cleaned Positive tweets = ['one', 'irresist', 'flipkartfashionfriday']\n"
     ]
    }
   ],
   "source": [
    "# print cleaned tweet\n",
    "print (\"Cleaned Tweets =\",clean_tweets(custom_tweet))\n",
    "\n",
    "print (\"Positive tweets = \",pos_tweets[5])\n",
    " \n",
    "print (\"Cleaned Positive tweets =\",clean_tweets(pos_tweets[5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "We define a simple bag_of_words function that extracts unigram features from the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': True, 'great': True, 'day': True, 'good': True, 'morn': True}\n",
      "5000 5000\n"
     ]
    }
   ],
   "source": [
    "# feature extractor function\n",
    "def bag_of_words(tweet):\n",
    "    words = clean_tweets(tweet)\n",
    "    words_dictionary = dict([word, True] for word in words)    \n",
    "    return words_dictionary\n",
    " \n",
    "custom_tweet = \"RT @Twitter @finkd Hello There! Have a great day. :) #good #morning \"\n",
    "print (bag_of_words(custom_tweet))\n",
    "\n",
    "# positive tweets feature set\n",
    "pos_tweets_set = []\n",
    "for tweet in pos_tweets:\n",
    "    pos_tweets_set.append((bag_of_words(tweet), 'pos'))    \n",
    " \n",
    "# negative tweets feature set\n",
    "neg_tweets_set = []\n",
    "for tweet in neg_tweets:\n",
    "    neg_tweets_set.append((bag_of_words(tweet), 'neg'))\n",
    " \n",
    "print (len(pos_tweets_set), len(neg_tweets_set)) # Output: (5000, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Test Set\n",
    "There are 5000 positive tweets set and 5000 negative tweets set. We take 20% (i.e. 1000) of positive tweets and 20% (i.e. 1000) of negative tweets as the test set. The remaining negative and positive tweets will be taken as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 8000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# radomize pos_reviews_set and neg_reviews_set\n",
    "# doing so will output different accuracy result everytime we run the program\n",
    "from random import shuffle \n",
    "shuffle(pos_tweets_set)\n",
    "shuffle(neg_tweets_set)\n",
    " \n",
    "test_set = pos_tweets_set[:1000] + neg_tweets_set[:1000]\n",
    "train_set = pos_tweets_set[1000:] + neg_tweets_set[1000:]\n",
    " \n",
    "print(len(test_set),  len(train_set)) # Output: (2000, 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'aww': True, 'bestfriend': True, 'look': True}, 'pos')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Classifier and Calculating Accuracy\n",
    "We train Naive Bayes Classifier using the training set and calculate the classification accuracy of the trained classifier using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.7345\n",
      "Most Informative Features\n",
      "                     via = True              pos : neg    =     33.0 : 1.0\n",
      "                     bam = True              pos : neg    =     25.0 : 1.0\n",
      "                      ff = True              pos : neg    =     22.3 : 1.0\n",
      "                      aw = True              neg : pos    =     21.7 : 1.0\n",
      "                     sad = True              neg : pos    =     21.7 : 1.0\n",
      "                     x15 = True              neg : pos    =     21.0 : 1.0\n",
      "                    glad = True              pos : neg    =     15.0 : 1.0\n",
      "                  welcom = True              pos : neg    =     13.7 : 1.0\n",
      "                   didnt = True              neg : pos    =     13.7 : 1.0\n",
      "                opportun = True              pos : neg    =     13.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    " \n",
    "accuracy = classify.accuracy(classifier, test_set)\n",
    "print(\"Accuracy = \", accuracy) \n",
    " \n",
    "print (classifier.show_most_informative_features(10))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Classifier with Custom Tweet\n",
    "We provide custom tweet and check the classification output of the trained classifier. The classifier correctly predicts both negative and positive tweets provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n",
      "Classifier probability result = <ProbDist with 2 samples>\n",
      "Sentiment of Maximum probability =  neg\n",
      "Percentage of Maximum negative sentiment.=  89.1942040987131\n",
      "Percentage of Maximum positive sentiment.= 10.805795901286666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "custom_tweet = \"I hated the film. It was a disaster. Poor direction, bad acting.\"\n",
    "custom_tweet_set = bag_of_words(custom_tweet)\n",
    "print (classifier.classify(custom_tweet_set)) # Output: neg\n",
    "# Negative tweet correctly classified as negative\n",
    " \n",
    "# probability result\n",
    "prob_result = classifier.prob_classify(custom_tweet_set)\n",
    "print (\"Classifier probability result =\", prob_result) # Output: <ProbDist with 2 samples>\n",
    "print (\"Sentiment of Maximum probability = \",prob_result.max()) # Output: neg\n",
    "print (\"Percentage of Maximum negative sentiment.= \",prob_result.prob(\"neg\")*100)\n",
    "print (\"Percentage of Maximum positive sentiment.=\",prob_result.prob(\"pos\")*100)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n",
      "Classifier probability result = <ProbDist with 2 samples>\n",
      "Sentiment of Maximum probability =  pos\n",
      "Percentage of Maximum negative sentiment.=  0.05978548024480185\n",
      "Percentage of Maximum positive sentiment.= 99.94021451975524\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"It was a wonderful and amazing movie. I loved it. Best direction, good acting.\"\n",
    "custom_tweet_set = bag_of_words(custom_tweet)\n",
    " \n",
    "print (classifier.classify(custom_tweet_set)) # Output: pos\n",
    "# Positive tweet correctly classified as positive\n",
    " \n",
    "# probability result\n",
    "prob_result = classifier.prob_classify(custom_tweet_set)\n",
    "print (\"Classifier probability result =\", prob_result) # Output: <ProbDist with 2 samples>\n",
    "print (\"Sentiment of Maximum probability = \",prob_result.max()) # Output: pos\n",
    "print (\"Percentage of Maximum negative sentiment.= \",prob_result.prob(\"neg\")*100)\n",
    "print (\"Percentage of Maximum positive sentiment.=\",prob_result.prob(\"pos\")*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall & F1-Score\n",
    "Accuracy is (correctly predicted observation) / (total observation).\n",
    "\n",
    "Precision is about being precise. <br>\n",
    "– It shows how many correct predictions were given. <br>\n",
    "– For example, out of 100 questions, if you answered only 1 question and answered it correctly then you will have 100% precision. <br>\n",
    "– It’s about checking how often the classifier predicts the result correctly. <br>\n",
    "\n",
    "Recall (as opposed to precision) <br>\n",
    "– is about answering all questions that have the answer “true” with the answer “true”. <br>\n",
    "– It’s about checking how often does the classifier predicts “yes” when the result is actually “yes”. <br>\n",
    "\n",
    "F1 Score or F-measure: Harmonic mean of recall and precision.\n",
    "\n",
    "We should have “true” answers and “false” answers for the calculation of precision and recall.\n",
    "\n",
    "For mathematical representation of precision and recall, we need to understand the following:\n",
    "\n",
    "True Positive (TP): e.g. the number of patients who did have cancer whom we correctly diagnosed as having cancer\n",
    "True Negative (TN): e.g. the number of patients who did not have cancer whom we correctly diagnosed as not having cancer\n",
    "\n",
    "False Positive (FP): e.g. the number of patients who did not have cancer whom we incorrectly diagnosed as having cancer (Also known as Type I error)\n",
    "False Negative (FN): e.g. the number of patients who did have cancer whom we incorrectly diagnosed as not having cancer (Also known as Type II error)\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "Precision = (TP) / (TP + FP)\n",
    "\n",
    "Recall = (TP) / (TP + FN)\n",
    "\n",
    "F1 Score = 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos precision: 0.7274490785645005\n",
      "pos recall: 0.75\n",
      "pos F-measure: 0.7385524372230428\n",
      "neg precision: 0.7420020639834881\n",
      "neg recall: 0.719\n",
      "neg F-measure: 0.7303199593702387\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    " \n",
    "actual_set = defaultdict(set)\n",
    "predicted_set = defaultdict(set)\n",
    " \n",
    "actual_set_cm = []\n",
    "predicted_set_cm = []\n",
    " \n",
    "for index, (feature, actual_label) in enumerate(test_set):\n",
    "    actual_set[actual_label].add(index)\n",
    "    actual_set_cm.append(actual_label)\n",
    " \n",
    "    predicted_label = classifier.classify(feature)\n",
    " \n",
    "    predicted_set[predicted_label].add(index)\n",
    "    predicted_set_cm.append(predicted_label)\n",
    "    \n",
    "from nltk.metrics import precision, recall, f_measure, ConfusionMatrix\n",
    " \n",
    "print ('pos precision:', precision(actual_set['pos'], predicted_set['pos'])) # Output: pos precision: 0.762896825397\n",
    "print ('pos recall:', recall(actual_set['pos'], predicted_set['pos'])) # Output: pos recall: 0.769\n",
    "print ('pos F-measure:', f_measure(actual_set['pos'], predicted_set['pos'])) # Output: pos F-measure: 0.76593625498\n",
    " \n",
    "print ('neg precision:', precision(actual_set['neg'], predicted_set['neg'])) # Output: neg precision: 0.767137096774\n",
    "print ('neg recall:', recall(actual_set['neg'], predicted_set['neg'])) # Output: neg recall: 0.761\n",
    "print ('neg F-measure:', f_measure(actual_set['neg'], predicted_set['neg'])) # Output: neg F-measure: 0.7640562249"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   n   p |\n",
      "    |   e   o |\n",
      "    |   g   s |\n",
      "----+---------+\n",
      "neg |<719>281 |\n",
      "pos | 250<750>|\n",
      "----+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = ConfusionMatrix(actual_set_cm, predicted_set_cm)\n",
    "print (cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |      n      p |\n",
      "    |      e      o |\n",
      "    |      g      s |\n",
      "----+---------------+\n",
      "neg | <36.0%> 14.1% |\n",
      "pos |  12.5% <37.5%>|\n",
      "----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
